@article{10.1145/3341110,
author = {Liu, Dayiheng and Xue, Yang and He, Feng and Chen, Yuanyuan and Lv, Jiancheng},
title = {Î¼-Forcing: Training Variational Recurrent Autoencoders for Text Generation},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3341110},
doi = {10.1145/3341110},
abstract = {It has been previously observed that training Variational Recurrent Autoencoders (VRAE) for text generation suffers from serious uninformative latent variables problems. The model would collapse into a plain language model that totally ignores the latent variables and can only generate repeating and dull samples. In this article, we explore the reason behind this issue and propose an effective regularizer-based approach to address it. The proposed method directly injects extra constraints on the posteriors of latent variables into the learning process of VRAE, which can flexibly and stably control the tradeoff between the Kullback-Leibler (KL) term and the reconstruction term, making the model learn dense and meaningful latent representations. The experimental results show that the proposed method outperforms several strong baselines and can make the model learn interpretable latent variables and generate diverse meaningful sentences. Furthermore, the proposed method can perform well without using other strategies, such as KL annealing.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {12},
numpages = {17},
keywords = {variational recurrent autoencoders, uninformative latent variables issues, Variational autoencoders}
}